{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<center><span style=\n",
    "  \"\n",
    "  font-size: 80px; \n",
    "  font-weight: bold;\n",
    "  color: Yellow;\n",
    "  text-decoration: underline;\n",
    "  text-decoration-color: White;\n",
    "  \"\n",
    ">\n",
    "   Deep Learning <!--  paste your text -->\n",
    "  </span></center>\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><span style=\n",
    "  \"\n",
    "  font-size: 50px; \n",
    "  font-weight: bold;\n",
    "  color: Yellow;\n",
    "  text-decoration: underline;\n",
    "  text-decoration-color: White;\n",
    "  \"\n",
    ">\n",
    "   Activation Functions <!--  paste your text -->\n",
    "  </span></center>\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "| Activation Function | Common Use | Advantages | Disadvantages |\n",
    "|---|---|---|---|\n",
    "| Sigmoid / Logistic | Predicting the probability as output | Outputs between 0 and 1, useful for binary classification, differentiable and provides smooth gradient | Suffers from vanishing gradient problem, output not symmetric around zero |\n",
    "| Tanh (Hyperbolic Tangent) | Hidden layers of neural network | Output is zero-centered, helps centering the data | Suffers from vanishing gradient problem, gradient is steeper compared to sigmoid |\n",
    "| ReLU (Rectified Linear Unit) | Hidden layers of neural network | Computationally efficient, accelerates convergence of gradient descent | Suffers from \"dying ReLU\" problem, all negative input values become zero |\n",
    "| Leaky ReLU | To avoid dying ReLU problem | Enables backpropagation for negative input values, avoids dead neurons | Predictions may not be consistent for negative input values, learning of model parameters is time-consuming |\n",
    "| Parametric ReLU | When Leaky ReLU fails at solving dead neurons problem | Slope of the negative part can be learnt during backpropagation | Performance varies depending on the value of slope parameter 'a' |\n",
    "| Exponential Linear Units (ELUs) | Modifies slope of the negative part of the function | Smoothly approaches the value of -Î± for negative inputs, avoids dead ReLU problem | Increases computational time, no learning of the 'a' value, suffers from exploding gradient problem |\n",
    "| Softmax | Output layer of classifier to represent probability distribution | Output is a probability distribution over 'n' classes | Limitations when dealing with non-exclusive classes |\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "  text-align: left;\n",
    "  font-size: 25px;\n",
    "  font-weight: bold;\n",
    "  color: Orange;\n",
    "  text-decoration: underline;\n",
    "  text-decoration-color: White;\n",
    "\">\n",
    "  Let's Make a Neural Network Using Tensorflow <!-- Paste your text -->\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "7/7 [==============================] - 1s 74ms/step - loss: 11.4157\n",
      "Epoch 2/50\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 10.5062\n",
      "Epoch 3/50\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 9.6338\n",
      "Epoch 4/50\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 8.7417\n",
      "Epoch 5/50\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 7.7804\n",
      "Epoch 6/50\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 6.7439\n",
      "Epoch 7/50\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 5.7072\n",
      "Epoch 8/50\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 4.6996\n",
      "Epoch 9/50\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 3.7934\n",
      "Epoch 10/50\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 3.0834\n",
      "Epoch 11/50\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 2.5642\n",
      "Epoch 12/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 2.3180\n",
      "Epoch 13/50\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 2.1791\n",
      "Epoch 14/50\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 2.1097\n",
      "Epoch 15/50\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 2.0395\n",
      "Epoch 16/50\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.9752\n",
      "Epoch 17/50\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.9159\n",
      "Epoch 18/50\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.8523\n",
      "Epoch 19/50\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.7946\n",
      "Epoch 20/50\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.7417\n",
      "Epoch 21/50\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.6897\n",
      "Epoch 22/50\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.6457\n",
      "Epoch 23/50\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.6185\n",
      "Epoch 24/50\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.5832\n",
      "Epoch 25/50\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.5510\n",
      "Epoch 26/50\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.5158\n",
      "Epoch 27/50\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.4707\n",
      "Epoch 28/50\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.4304\n",
      "Epoch 29/50\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.4069\n",
      "Epoch 30/50\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.3922\n",
      "Epoch 31/50\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.3691\n",
      "Epoch 32/50\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.3413\n",
      "Epoch 33/50\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.3238\n",
      "Epoch 34/50\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.3197\n",
      "Epoch 35/50\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.3095\n",
      "Epoch 36/50\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.2917\n",
      "Epoch 37/50\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.2769\n",
      "Epoch 38/50\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.2675\n",
      "Epoch 39/50\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.2590\n",
      "Epoch 40/50\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.2555\n",
      "Epoch 41/50\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.2421\n",
      "Epoch 42/50\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.2285\n",
      "Epoch 43/50\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.2321\n",
      "Epoch 44/50\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.2311\n",
      "Epoch 45/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.2251\n",
      "Epoch 46/50\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.2087\n",
      "Epoch 47/50\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.2004\n",
      "Epoch 48/50\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.2021\n",
      "Epoch 49/50\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.1980\n",
      "Epoch 50/50\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.1918\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.6861\n",
      "Test Loss: 0.6860761642456055\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "\n",
    "# Load the tips dataset\n",
    "tips = sns.load_dataset(\"tips\")\n",
    "\n",
    "# label encode the data\n",
    "le = LabelEncoder()\n",
    "tips['sex'] = le.fit_transform(tips['sex'])\n",
    "tips['smoker'] = le.fit_transform(tips['smoker'])\n",
    "tips['day'] = le.fit_transform(tips['day'])\n",
    "tips['time'] = le.fit_transform(tips['time'])\n",
    "\n",
    "# Preprocess the data\n",
    "X = tips.drop('tip', axis=1)\n",
    "y = tips['tip']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features using MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create a simple neural network model\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(32, activation='relu', input_shape=(6,)),\n",
    "    keras.layers.Dense(16, activation='relu'),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X_test_scaled, y_test)\n",
    "print('Test Loss:', loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.12.0\n",
      "Num GPUs Available:  1\n",
      "WARNING:tensorflow:AutoGraph could not transform <function normalize_img at 0x28914fb50> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function normalize_img at 0x28914fb50>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function normalize_img at 0x28914fb50> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function normalize_img at 0x28914fb50>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function normalize_img at 0x28914fb50> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function normalize_img at 0x28914fb50>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Epoch 1/12\n",
      "469/469 [==============================] - 9s 16ms/step - loss: 0.1662 - accuracy: 0.9506 - val_loss: 0.0515 - val_accuracy: 0.9843\n",
      "Epoch 2/12\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.0444 - accuracy: 0.9863 - val_loss: 0.0415 - val_accuracy: 0.9867\n",
      "Epoch 3/12\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.0293 - accuracy: 0.9907 - val_loss: 0.0316 - val_accuracy: 0.9901\n",
      "Epoch 4/12\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.0178 - accuracy: 0.9944 - val_loss: 0.0413 - val_accuracy: 0.9864\n",
      "Epoch 5/12\n",
      "469/469 [==============================] - 6s 14ms/step - loss: 0.0135 - accuracy: 0.9954 - val_loss: 0.0334 - val_accuracy: 0.9898\n",
      "Epoch 6/12\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.0096 - accuracy: 0.9971 - val_loss: 0.0428 - val_accuracy: 0.9872\n",
      "Epoch 7/12\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.0091 - accuracy: 0.9971 - val_loss: 0.0343 - val_accuracy: 0.9903\n",
      "Epoch 8/12\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.0053 - accuracy: 0.9983 - val_loss: 0.0335 - val_accuracy: 0.9907\n",
      "Epoch 9/12\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.0054 - accuracy: 0.9983 - val_loss: 0.0413 - val_accuracy: 0.9886\n",
      "Epoch 10/12\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.0037 - accuracy: 0.9988 - val_loss: 0.0438 - val_accuracy: 0.9902\n",
      "Epoch 11/12\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.0060 - accuracy: 0.9981 - val_loss: 0.0451 - val_accuracy: 0.9892\n",
      "Epoch 12/12\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.0027 - accuracy: 0.9991 - val_loss: 0.0569 - val_accuracy: 0.9869\n",
      "CPU times: user 49.1 s, sys: 25.8 s, total: 1min 14s\n",
      "Wall time: 1min 18s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x28916fd90>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "tf.config.list_physical_devices('GPU')\n",
    "(ds_train, ds_test), ds_info = tfds.load(\n",
    "    'mnist',\n",
    "    split=['train', 'test'],\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True,\n",
    "    with_info=True,\n",
    ")\n",
    "def normalize_img(image, label):\n",
    "  \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n",
    "  return tf.cast(image, tf.float32) / 255., label\n",
    "batch_size = 128\n",
    "ds_train = ds_train.map(\n",
    "    normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "ds_train = ds_train.cache()\n",
    "ds_train = ds_train.shuffle(ds_info.splits['train'].num_examples)\n",
    "ds_train = ds_train.batch(batch_size)\n",
    "ds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "ds_test = ds_test.map(\n",
    "    normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "ds_test = ds_test.batch(batch_size)\n",
    "ds_test = ds_test.cache()\n",
    "ds_test = ds_test.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu'),\n",
    "  tf.keras.layers.Conv2D(64, kernel_size=(3, 3),\n",
    "                 activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "#   tf.keras.layers.Dropout(0.25),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "#   tf.keras.layers.Dropout(0.5),\n",
    "  tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.legacy.Adam(0.001),\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "model.fit(\n",
    "    ds_train,\n",
    "    epochs=12,\n",
    "    validation_data=ds_test,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's make a simple Artificial neural network (ANN) using Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 11.5564\n",
      "Epoch 2/50\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 11.3328\n",
      "Epoch 3/50\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 11.1778\n",
      "Epoch 4/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 11.0376\n",
      "Epoch 5/50\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 10.8830\n",
      "Epoch 6/50\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 10.7204\n",
      "Epoch 7/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 10.5504\n",
      "Epoch 8/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 10.3609\n",
      "Epoch 9/50\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 10.1312\n",
      "Epoch 10/50\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 9.8638\n",
      "Epoch 11/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 9.5549\n",
      "Epoch 12/50\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 9.2088\n",
      "Epoch 13/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 8.8070\n",
      "Epoch 14/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 8.3716\n",
      "Epoch 15/50\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 7.8990\n",
      "Epoch 16/50\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 7.3902\n",
      "Epoch 17/50\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 6.8396\n",
      "Epoch 18/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 6.2343\n",
      "Epoch 19/50\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 5.6172\n",
      "Epoch 20/50\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 4.9908\n",
      "Epoch 21/50\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 4.3892\n",
      "Epoch 22/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 3.7889\n",
      "Epoch 23/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 3.2497\n",
      "Epoch 24/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 2.7853\n",
      "Epoch 25/50\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 2.3982\n",
      "Epoch 26/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 2.1405\n",
      "Epoch 27/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.9628\n",
      "Epoch 28/50\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.8559\n",
      "Epoch 29/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.7837\n",
      "Epoch 30/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.7382\n",
      "Epoch 31/50\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.7018\n",
      "Epoch 32/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.6707\n",
      "Epoch 33/50\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.6541\n",
      "Epoch 34/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.6307\n",
      "Epoch 35/50\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.6145\n",
      "Epoch 36/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.5978\n",
      "Epoch 37/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.5819\n",
      "Epoch 38/50\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.5686\n",
      "Epoch 39/50\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 1.5522\n",
      "Epoch 40/50\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.5406\n",
      "Epoch 41/50\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.5293\n",
      "Epoch 42/50\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 1.5169\n",
      "Epoch 43/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.5069\n",
      "Epoch 44/50\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.4936\n",
      "Epoch 45/50\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.4824\n",
      "Epoch 46/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.4735\n",
      "Epoch 47/50\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 1.4633\n",
      "Epoch 48/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.4562\n",
      "Epoch 49/50\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.4497\n",
      "Epoch 50/50\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.4419\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.9411\n",
      "Test Loss: 0.9410595297813416\n"
     ]
    }
   ],
   "source": [
    "# Step: 1 Import libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "\n",
    "# dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# scikit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "# import the dataset\n",
    "df = sns.load_dataset(\"tips\")\n",
    "\n",
    "# preprocess the data\n",
    "le = LabelEncoder()\n",
    "df['sex'] = le.fit_transform(df['sex'])\n",
    "df['smoker'] = le.fit_transform(df['smoker'])\n",
    "df['day'] = le.fit_transform(df['day'])\n",
    "df['time'] = le.fit_transform(df['time'])\n",
    "\n",
    "X = df.drop(\"tip\", axis=1)\n",
    "y = df[\"tip\"]\n",
    "\n",
    "# scale the data\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# train test split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Creata a model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(16, activation='relu', input_shape=[X_train.shape[1]]), # input layer\n",
    "    tf.keras.layers.Dense(8, activation='relu'), # hidden layer\n",
    "    tf.keras.layers.Dense(1) # output layer\n",
    "])\n",
    "# compile the model\n",
    "model.compile(\n",
    "    optimizer='adam', loss='mean_squared_error'\n",
    ")\n",
    "# train the model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=1)\n",
    "# evaluate the model\n",
    "loss = model.evaluate(X_test, y_test)\n",
    "print('Test Loss:', loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "7/7 [==============================] - 1s 30ms/step - loss: 0.6402 - accuracy: 0.7128\n",
      "Epoch 2/5\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.6333 - accuracy: 0.7128\n",
      "Epoch 3/5\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.6269 - accuracy: 0.7128\n",
      "Epoch 4/5\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 0.6224 - accuracy: 0.7128\n",
      "Epoch 5/5\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.6182 - accuracy: 0.7128\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.7046 - accuracy: 0.5510\n",
      "Test Loss: 0.7046390175819397\n",
      "Test Accuracy: 0.5510203838348389\n"
     ]
    }
   ],
   "source": [
    "# Step: 1 Import libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "\n",
    "# dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# scikit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "\n",
    "# import the dataset\n",
    "df = sns.load_dataset(\"tips\")\n",
    "\n",
    "# preprocess the data\n",
    "le = LabelEncoder()\n",
    "df['sex'] = le.fit_transform(df['sex'])\n",
    "df['smoker'] = le.fit_transform(df['smoker'])\n",
    "df['day'] = le.fit_transform(df['day'])\n",
    "df['time'] = le.fit_transform(df['time'])\n",
    "\n",
    "# convert 'tip' to a binary variable, 1 if the tip is above $2, else 0\n",
    "df['tip'] = (df['tip'] > 2).astype(int)\n",
    "\n",
    "X = df.drop(\"tip\", axis=1)\n",
    "y = df[\"tip\"]\n",
    "\n",
    "# scale the data\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# train test split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Creata a model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(16, activation='relu', input_shape=[X_train.shape[1]]), # input layer + Hidden layer\n",
    "    tf.keras.layers.Dense(8, activation='relu'), # hidden layer\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid') # output layer\n",
    "])\n",
    "\n",
    "# compile the model\n",
    "model.compile(\n",
    "    optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# train the model\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=32, verbose=1) # verbose = [0,1,2], Batch_size=number_of_samples in one iteration\n",
    "\n",
    "\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print('Test Loss:', loss)\n",
    "print('Test Accuracy:', accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.12.0\n"
     ]
    }
   ],
   "source": [
    "# find tensorflow version\n",
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
